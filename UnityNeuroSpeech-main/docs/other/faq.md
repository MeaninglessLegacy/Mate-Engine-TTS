# â“ FAQ

---

## âš™ï¸ General

---

### Why use Coqui XTTS as the TTS model?

Three main reasons:

- **Custom Voices**.

    You can use any voice you want: your own, Cyn, Vito Corleone, Justin Timberlake â€” whatever.
I believe thatâ€™s way more flexible than static, pre-generated voices.

- **License**.

    Some TTS models have extremely restrictive licenses. For example, Silero TTS is under CC BY-NC-SA 4.0, which means:
    - You canâ€™t use it commercially.
    - Even worse: Iâ€™d be forced to release this entire framework under the same license.
Thatâ€™s a no-go.
  
- **Simplicity**.

    The Python server is **just ~100 lines of code**.
It supports 10+ languages out of the box.
You send a UnityWebRequest with text â€” it sends back .wav.
Thatâ€™s it. No ONNX, no weird tensors, no converting text into token IDs, etc.


But if you hate Python or Coqui XTTS, you can integrate your favorite model.
Modularity is the point. I wonâ€™t stop you ğŸ¤ 

---

## ğŸ Python

---

### Why use Python for TTS?

I didnâ€™t really have a choice.

**Coqui XTTS only offers a Python API**, and there are no stable C#, C++, C, TypeScript, JavaScript or Go bindings.
If there had been a solid alternative in another language, I wouldâ€™ve gladly used it â€” but this is what weâ€™ve got.

---

### Why is Python.exe using so much RAM?
Because itâ€™s running a full neural TTS model â€” locally â€” in Python.
Thatâ€™s the price you pay for realistic voices and multilingual support.
If it bothers you â€” try switching to smaller models or optimize the server logic yourself.

---

## â— Most Important Question

---

### Why is it so hard to configure and get started?

A totally fair question â€” especially after youâ€™ve gone through all the setup steps and seen how many pieces are involved.

Hereâ€™s the truth:
> UnityNeuroSpeech is the first â€” not just Unity, but game development framework â€” that allows you to talk to AI directly inside your game.

---

The only out-of-the-box solution for Unity in this whole stack is [whisper.unity](https://github.com/Macoron/whisper.unity). 
And even then â€” you still need to create a separate `SetupWhisperPath.cs` script to make it work properly in builds. Yes, you can try using **StreamingAssets**, and whisper.unity supports that.
But I personally prefer not to scatter files across multiple Unity folders.
If thatâ€™s critical for your project â€” feel free to try it with StreamingAssets.

---

Now letâ€™s talk about Text-to-Speech...

Frankly, there are no really â€œcleanâ€ or â€œeasyâ€ TTS solutions â€” not just for Unity, **but even for C# as a whole**.
(If youâ€™re curious about the deep rabbit hole of TTS licensing and tech, check out the [Python Server](server.md) page.)

---

As for **Ollama**, I integrated it into Unity myself â€” using Microsoftâ€™s `Microsoft.Extensions` libraries.

---

Yes â€” it does feel like a lot at first.

But donâ€™t worry â€” **UnityNeuroSpeech is actively being improved**, and with every update, the setup process will become simpler and more automated.

---

And maybe one day, this kind of tech will be built into Unity out of the box.
Youâ€™ll click a button, and boom â€” talking AI agents everywhere.

But when that day comes... **it wonâ€™t be unique anymore.**

So if you want to stand out with a game that features **real, emotional voice interaction powered by AI** â€” nowâ€™s your chance.
Go for it ğŸ˜